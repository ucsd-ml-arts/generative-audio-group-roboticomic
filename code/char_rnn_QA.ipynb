{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mainly copied and adapted from the Text Generation RNN example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "path_to_file = \"jokes.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 3288227 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n",
      "i\n",
      "d\n",
      " \n",
      "y\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Did you hear about the Native American man that drank 200 cups of tea?,He nearly drown in his own tea'\n",
      "\" pee.\\r\\nWhat's the best anti diarrheal prescription?,Mycheexarphlexin\\r\\nWhat do you call a person who i\"\n",
      "'s outside a door and has no arms nor legs?,Matt\\r\\nWhich Star Trek character is a member of the magic c'\n",
      "\"ircle?,Jean-Luc Pickacard\\r\\nWhat's the difference between a bullet and a human?,A bullet doesn't miss \"\n",
      "\"Harambe\\r\\nWhy was the Ethiopian baby crying?,He was having a mid-life crisis\\r\\nWhat's the difference be\"\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available(): #GPU\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    # tack on two RNN layers \n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    \n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "      \n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 239) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           61184     \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (64, None, 1024)          6297600   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (64, None, 1024)          6297600   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 239)           244975    \n",
      "=================================================================\n",
      "Total params: 16,839,663\n",
      "Trainable params: 16,839,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 239)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       5.47675\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "#   return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './text100'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 12/513 [..............................] - ETA: 1:51 - loss: 3.2904"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bd6331f2e2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1612\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1615\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m       return training_distributed.fit_loop(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    703\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m           \u001b[0mdo_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m           batch_size=batch_size)\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36miterator_fit_loop\u001b[0;34m(model, inputs, class_weight, steps_per_epoch, epoch_logs, val_inputs, val_targets, val_sample_weights, epochs, verbose, callbacks, validation_steps, do_validation, batch_size)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Train model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     outs, loss, loss_metrics, masks = _process_single_batch(\n\u001b[0;32m--> 251\u001b[0;31m         model, x, y, sample_weights=sample_weights, training=True)\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, sample_weights, training)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         model.optimizer.apply_gradients(zip(grads,\n\u001b[0;32m--> 523\u001b[0;31m                                             model._collected_trainable_weights))\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    608\u001b[0m           \u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m           \u001b[0mupdate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mupdate_op\u001b[0;34m(self, optimizer, g)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \"Cannot use a constraint function on a sparse variable.\")\n\u001b[1;32m    165\u001b[0m       return optimizer._resource_apply_sparse_duplicate_indices(\n\u001b[0;32m--> 166\u001b[0;31m           g.values, self._v, g.indices)\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_resource_apply_sparse_duplicate_indices\u001b[0;34m(self, grad, handle, indices)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \"\"\"\n\u001b[1;32m    961\u001b[0m     summed_grad, unique_indices = _deduplicate_indexed_slices(\n\u001b[0;32m--> 962\u001b[0;31m         values=grad, indices=indices)\n\u001b[0m\u001b[1;32m    963\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummed_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_deduplicate_indexed_slices\u001b[0;34m(values, indices)\u001b[0m\n\u001b[1;32m     74\u001b[0m   summed_values = math_ops.unsorted_segment_sum(\n\u001b[1;32m     75\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index_positions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       array_ops.shape(unique_indices)[0])\n\u001b[0m\u001b[1;32m     77\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msummed_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36munsorted_segment_sum\u001b[0;34m(data, segment_ids, num_segments, name)\u001b[0m\n\u001b[1;32m   9050\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9051\u001b[0m         \u001b[0;34m\"UnsortedSegmentSum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9052\u001b[0;31m         segment_ids, num_segments)\n\u001b[0m\u001b[1;32m   9053\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9054\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-66c5d1dbc8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./text100/ckpt_31'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1) \n",
    "\n",
    "#model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.load_weights('./text100/ckpt_31')\n",
    "\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (1, None, 256)            61184     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_12 (CuDNNGRU)      (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_13 (CuDNNGRU)      (1, None, 1024)           6297600   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_14 (CuDNNGRU)      (1, None, 1024)           6297600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, None, 239)            244975    \n",
      "=================================================================\n",
      "Total params: 16,839,663\n",
      "Trainable params: 16,839,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 4500\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  temperature = 0.8 #reduce temperature to 0.7 non words appear at higher temperatures \n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Einstein?,A cereman cry\r\n",
      "Did you hear that 7.12 inches long?,Nuts. She's going to except myself.\r\n",
      "Why does the Energizer Bunny go to Jail?,He's all right not.\r\n",
      "What is and Saithrait's stafficient's Etherste Opponents\r\n",
      "What's a transvestite?,A: You're 10/200$=/2\r\n",
      "Why do jewish people hate sunblack tires?,They're always married Moundae- happened orgh.\r\n",
      "What's the difference between a prostitute and a puppy?,One is a crusty bus station and the other is a simplet sick?,A: A cow's cub\r\n",
      "What did the Irish guy do when he has a small off\r\n",
      "What's the difference between a 465 post and a 10-ide?,Orange Jews\r\n",
      "Why did the hipster cross the road?,To get to the same drop.\r\n",
      "How many cops does it take to screw in a light bulb?,None. They'll just charged them.\r\n",
      "What does a bride wear?,Astro\r\n",
      "How do you triggers clearly?,Eripéchett\r\n",
      "What does Obama's Assanting 50% off the door?,Islamic gorize\r\n",
      "What do the Polak and MIT?,\"A: Free Guts is a stupidest, meaning that audition?,I'll put their luctoritsuse.\r\n",
      "What's the difference between a post and a stook?,\"Ask reproductions, they're all pro-for 2086(\r\n",
      "What do you call a Greek arsonist?,Assault and Iraqis :/\r\n",
      "What's a pirate's least favorite drink?,Ass-co-Gips\r\n",
      "What's the worst thing you can do in a pig?,An ich-larger.\r\n",
      "How does a blonde chocolate operation?,She had a deaf prison?,\"He was in teaching, she pleased. \"\r\n",
      "What does Donald Trump's point have in common with a proper toast?,Re-part\r\n",
      "What's the difference between a Freshle and a Spanish T-Slutgy hotel have in common?,Not done.\r\n",
      "Where do senior got puzzed on her?,[removed]\r\n",
      "What's the worst part about having sex with 28-year-olds?,There's 20 of them\r\n",
      "What did the roadit's cannibal-esh-say when he ran into the sautural alone.\r\n",
      "What did the psychologist say to the rich?,They're making headlines.\r\n",
      "You know what's about the potato that got arthrused?,It had no balls!\r\n",
      "How do you spell Falloner?,You put them over there...\r\n",
      "Did you hear about the guy who traes to shut up?,It was soda-pressing. \r\n",
      "What does a cat have in common?,Both are the dads jokes (insert rating suspenders]\r\n",
      "What's the difference between a Greyhound terrorist and a 3/5th?,ACTantas\r\n",
      "How does a slavery come home?,You're bout hu\r\n",
      "What's a pessimists favorite American?,\"Do Yonky, org there aren't anyone complete.\r\n",
      "Need working jokes/can tell me?,\"I'm sure she starts t her-pyriated Nazi?,A: It's already did.\r\n",
      "What's the difference between the amputees and publicly 1000 visions\r\n",
      "Whats the difference between James Bonds and Möserancia?,A Scrack Abortion.\r\n",
      "How do you stop an elephant in a black guy proper-tea issued.\r\n",
      "What's Big?,hahhave her.\r\n",
      "What does a musician's find operating system?,Shit.\r\n",
      "What happens when a quarter politicians have in common?,If they're there lil...\r\n",
      "What's the difference between a Ferrari and a Cs. Rapper?,It's all her prostitutes\r\n",
      "How many NRA candy?,An undercover.\r\n",
      "What does a prostitute say to the prisoner?,Short fucking both.\r\n",
      "What's the similarity between RForm's soda's head?,Dogs to put their hair\r\n",
      "What's the worst part about locking your keys in your car in front of a cat?,A: A short toilet.\r\n",
      "What's the difference between a Priest and acne?,Awish manachutes\r\n",
      "Are you the Navy training Nords were together?,Muscletov.\r\n",
      "What's the difference between a brick and an Italian adouth of a horror from the starshot?,Every time consuming.\r\n",
      "Q: What do you call a smart musician?,A MaBMANAS!\r\n",
      "What's the most popular guy outside?,Ever made that up.\r\n",
      "What do you call a martinist who just got off of his head?,Because he is a shitty server.\r\n",
      "How many NSA should do it.\r\n",
      "What's the difference between a tribal tanksget?,Doesn't.\r\n",
      "What's the difference between a black acceptable and a fruit?,The police has a high school massage stick\r\n",
      "What's the heartside of a Terrorist?,It's full of seamen.\r\n",
      "What's the difference between a fireman and a hooker?,A homesave manaver\r\n",
      "Why are Asians studios?,It's missing the streets of headphones\r\n",
      "Everyone wants to foremputees around sexual.\r\n",
      "What did the vote of history team say in his phole?,He had a stroke back.\r\n",
      "How do you find With a Superman?,The hooker can washed his hamburger?,About half-Asian.\r\n",
      "Why does Buddhist man so odd?,Because he's there...\r\n",
      "Whats the difference between a feminist and a whore?,\"A rooster says \"\"cock-a-doodle-doo\"\", then the man: the good 2 into the human being pissed, I'm pretty she's over there..\"\r\n",
      "What's the start?,I'll get my wife't believe.\r\n",
      "How many creationists does it take to screw in a lightbulb\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Why\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.4756\n",
      "Epoch 1 Batch 100 Loss 2.1534\n",
      "Epoch 1 Batch 200 Loss 1.7199\n",
      "Epoch 1 Batch 300 Loss 1.5372\n",
      "Epoch 1 Batch 400 Loss 1.3819\n",
      "Epoch 1 Batch 500 Loss 1.4110\n",
      "Epoch 1 Loss 1.4006\n",
      "Time taken for 1 epoch 66.01726245880127 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.3780\n",
      "Epoch 2 Batch 100 Loss 1.3245\n",
      "Epoch 2 Batch 200 Loss 1.2854\n",
      "Epoch 2 Batch 300 Loss 1.3153\n",
      "Epoch 2 Batch 400 Loss 1.2544\n",
      "Epoch 2 Batch 500 Loss 1.2235\n",
      "Epoch 2 Loss 1.1820\n",
      "Time taken for 1 epoch 69.62842106819153 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.1716\n",
      "Epoch 3 Batch 100 Loss 1.2615\n",
      "Epoch 3 Batch 200 Loss 1.2031\n",
      "Epoch 3 Batch 300 Loss 1.2094\n",
      "Epoch 3 Batch 400 Loss 1.1266\n",
      "Epoch 3 Batch 500 Loss 1.1499\n",
      "Epoch 3 Loss 1.1936\n",
      "Time taken for 1 epoch 66.08210706710815 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.1485\n",
      "Epoch 4 Batch 100 Loss 1.1119\n",
      "Epoch 4 Batch 200 Loss 1.1851\n",
      "Epoch 4 Batch 300 Loss 1.1216\n",
      "Epoch 4 Batch 400 Loss 1.0891\n",
      "Epoch 4 Batch 500 Loss 1.1452\n",
      "Epoch 4 Loss 1.0942\n",
      "Time taken for 1 epoch 67.03789377212524 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.0741\n",
      "Epoch 5 Batch 100 Loss 1.0549\n",
      "Epoch 5 Batch 200 Loss 1.1447\n",
      "Epoch 5 Batch 300 Loss 1.0795\n",
      "Epoch 5 Batch 400 Loss 1.0854\n",
      "Epoch 5 Batch 500 Loss 1.0695\n",
      "Epoch 5 Loss 1.0624\n",
      "Time taken for 1 epoch 67.09503507614136 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.0274\n",
      "Epoch 6 Batch 100 Loss 1.0812\n",
      "Epoch 6 Batch 200 Loss 1.0394\n",
      "Epoch 6 Batch 300 Loss 1.0585\n",
      "Epoch 6 Batch 400 Loss 1.1123\n",
      "Epoch 6 Batch 500 Loss 1.0207\n",
      "Epoch 6 Loss 1.0918\n",
      "Time taken for 1 epoch 68.4194769859314 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.0445\n",
      "Epoch 7 Batch 100 Loss 1.0887\n",
      "Epoch 7 Batch 200 Loss 1.0852\n",
      "Epoch 7 Batch 300 Loss 1.0431\n",
      "Epoch 7 Batch 400 Loss 1.0319\n",
      "Epoch 7 Batch 500 Loss 1.0288\n",
      "Epoch 7 Loss 1.0362\n",
      "Time taken for 1 epoch 68.76260709762573 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.9841\n",
      "Epoch 8 Batch 100 Loss 1.0065\n",
      "Epoch 8 Batch 200 Loss 1.0113\n",
      "Epoch 8 Batch 300 Loss 1.0598\n",
      "Epoch 8 Batch 400 Loss 1.0224\n",
      "Epoch 8 Batch 500 Loss 1.0112\n",
      "Epoch 8 Loss 1.0112\n",
      "Time taken for 1 epoch 67.56926250457764 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.9630\n",
      "Epoch 9 Batch 100 Loss 0.9828\n",
      "Epoch 9 Batch 200 Loss 1.0069\n",
      "Epoch 9 Batch 300 Loss 0.9926\n",
      "Epoch 9 Batch 400 Loss 0.9831\n",
      "Epoch 9 Batch 500 Loss 0.9967\n",
      "Epoch 9 Loss 1.0018\n",
      "Time taken for 1 epoch 65.92713594436646 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.9531\n",
      "Epoch 10 Batch 100 Loss 0.9938\n",
      "Epoch 10 Batch 200 Loss 0.9366\n",
      "Epoch 10 Batch 300 Loss 0.9707\n",
      "Epoch 10 Batch 400 Loss 0.9330\n",
      "Epoch 10 Batch 500 Loss 0.9862\n",
      "Epoch 10 Loss 1.0088\n",
      "Time taken for 1 epoch 67.46083855628967 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.9291\n",
      "Epoch 11 Batch 100 Loss 0.9618\n",
      "Epoch 11 Batch 200 Loss 0.9942\n",
      "Epoch 11 Batch 300 Loss 0.9866\n",
      "Epoch 11 Batch 400 Loss 0.9484\n",
      "Epoch 11 Batch 500 Loss 0.9840\n",
      "Epoch 11 Loss 0.9673\n",
      "Time taken for 1 epoch 68.21954011917114 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.9392\n",
      "Epoch 12 Batch 100 Loss 0.9196\n",
      "Epoch 12 Batch 200 Loss 0.9240\n",
      "Epoch 12 Batch 300 Loss 0.9366\n",
      "Epoch 12 Batch 400 Loss 0.9572\n",
      "Epoch 12 Batch 500 Loss 0.9196\n",
      "Epoch 12 Loss 0.9647\n",
      "Time taken for 1 epoch 67.9308671951294 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.8950\n",
      "Epoch 13 Batch 100 Loss 0.8921\n",
      "Epoch 13 Batch 200 Loss 0.9301\n",
      "Epoch 13 Batch 300 Loss 0.9435\n",
      "Epoch 13 Batch 400 Loss 0.9384\n",
      "Epoch 13 Batch 500 Loss 0.9643\n",
      "Epoch 13 Loss 0.9171\n",
      "Time taken for 1 epoch 72.6405200958252 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.8702\n",
      "Epoch 14 Batch 100 Loss 0.8969\n",
      "Epoch 14 Batch 200 Loss 0.9477\n",
      "Epoch 14 Batch 300 Loss 0.9166\n",
      "Epoch 14 Batch 400 Loss 0.9586\n",
      "Epoch 14 Batch 500 Loss 0.9448\n",
      "Epoch 14 Loss 0.9517\n",
      "Time taken for 1 epoch 66.65447449684143 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.9057\n",
      "Epoch 15 Batch 100 Loss 0.9613\n",
      "Epoch 15 Batch 200 Loss 0.9236\n",
      "Epoch 15 Batch 300 Loss 0.9595\n",
      "Epoch 15 Batch 400 Loss 0.9467\n",
      "Epoch 15 Batch 500 Loss 0.9258\n",
      "Epoch 15 Loss 0.9150\n",
      "Time taken for 1 epoch 68.19959807395935 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.8542\n",
      "Epoch 16 Batch 100 Loss 0.9006\n",
      "Epoch 16 Batch 200 Loss 0.9095\n",
      "Epoch 16 Batch 300 Loss 0.8884\n",
      "Epoch 16 Batch 400 Loss 0.9277\n",
      "Epoch 16 Batch 500 Loss 0.9453\n",
      "Epoch 16 Loss 0.9880\n",
      "Time taken for 1 epoch 67.59258389472961 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.8175\n",
      "Epoch 17 Batch 100 Loss 0.9165\n",
      "Epoch 17 Batch 200 Loss 0.9206\n",
      "Epoch 17 Batch 300 Loss 0.9392\n",
      "Epoch 17 Batch 400 Loss 0.9924\n",
      "Epoch 17 Batch 500 Loss 0.9414\n",
      "Epoch 17 Loss 0.9181\n",
      "Time taken for 1 epoch 66.08769583702087 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.8812\n",
      "Epoch 18 Batch 100 Loss 0.9019\n",
      "Epoch 18 Batch 200 Loss 0.9103\n",
      "Epoch 18 Batch 300 Loss 0.8770\n",
      "Epoch 18 Batch 400 Loss 0.9524\n",
      "Epoch 18 Batch 500 Loss 0.9098\n",
      "Epoch 18 Loss 0.9428\n",
      "Time taken for 1 epoch 67.44419836997986 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.8963\n",
      "Epoch 19 Batch 100 Loss 0.9240\n",
      "Epoch 19 Batch 200 Loss 0.9414\n",
      "Epoch 19 Batch 300 Loss 0.8968\n",
      "Epoch 19 Batch 400 Loss 0.9298\n",
      "Epoch 19 Batch 500 Loss 0.9616\n",
      "Epoch 19 Loss 0.9200\n",
      "Time taken for 1 epoch 67.50806975364685 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.8777\n",
      "Epoch 20 Batch 100 Loss 0.9012\n",
      "Epoch 20 Batch 200 Loss 0.9274\n",
      "Epoch 20 Batch 300 Loss 0.9841\n",
      "Epoch 20 Batch 400 Loss 0.9156\n",
      "Epoch 20 Batch 500 Loss 0.9454\n",
      "Epoch 20 Loss 0.8813\n",
      "Time taken for 1 epoch 67.50882530212402 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.8867\n",
      "Epoch 21 Batch 100 Loss 0.8903\n",
      "Epoch 21 Batch 200 Loss 0.9077\n",
      "Epoch 21 Batch 300 Loss 0.9607\n",
      "Epoch 21 Batch 400 Loss 0.8925\n",
      "Epoch 21 Batch 500 Loss 0.9159\n",
      "Epoch 21 Loss 0.9118\n",
      "Time taken for 1 epoch 66.21242237091064 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.8702\n",
      "Epoch 22 Batch 100 Loss 0.9236\n",
      "Epoch 22 Batch 200 Loss 0.9015\n",
      "Epoch 22 Batch 300 Loss 0.9780\n",
      "Epoch 22 Batch 400 Loss 0.9342\n",
      "Epoch 22 Batch 500 Loss 0.9234\n",
      "Epoch 22 Loss 0.9625\n",
      "Time taken for 1 epoch 65.98595261573792 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.8960\n",
      "Epoch 23 Batch 100 Loss 0.8806\n",
      "Epoch 23 Batch 200 Loss 0.9615\n",
      "Epoch 23 Batch 300 Loss 0.8911\n",
      "Epoch 23 Batch 400 Loss 0.9535\n",
      "Epoch 23 Batch 500 Loss 0.9594\n",
      "Epoch 23 Loss 0.9553\n",
      "Time taken for 1 epoch 71.71443033218384 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.8699\n",
      "Epoch 24 Batch 100 Loss 0.9228\n",
      "Epoch 24 Batch 200 Loss 0.9528\n",
      "Epoch 24 Batch 300 Loss 0.9462\n",
      "Epoch 24 Batch 400 Loss 0.9386\n",
      "Epoch 24 Batch 500 Loss 0.9536\n",
      "Epoch 24 Loss 0.9493\n",
      "Time taken for 1 epoch 66.78715705871582 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.9158\n",
      "Epoch 25 Batch 100 Loss 0.9075\n",
      "Epoch 25 Batch 200 Loss 0.9461\n",
      "Epoch 25 Batch 300 Loss 0.9443\n",
      "Epoch 25 Batch 400 Loss 0.9270\n",
      "Epoch 25 Batch 500 Loss 0.9652\n",
      "Epoch 25 Loss 0.9415\n",
      "Time taken for 1 epoch 67.48568844795227 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.9310\n",
      "Epoch 26 Batch 100 Loss 0.9559\n",
      "Epoch 26 Batch 200 Loss 0.9615\n",
      "Epoch 26 Batch 300 Loss 0.9539\n",
      "Epoch 26 Batch 400 Loss 0.9647\n",
      "Epoch 26 Batch 500 Loss 0.9426\n",
      "Epoch 26 Loss 0.9999\n",
      "Time taken for 1 epoch 67.71436643600464 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.9028\n",
      "Epoch 27 Batch 100 Loss 0.9287\n",
      "Epoch 27 Batch 200 Loss 0.8927\n",
      "Epoch 27 Batch 300 Loss 0.9407\n",
      "Epoch 27 Batch 400 Loss 0.9811\n",
      "Epoch 27 Batch 500 Loss 0.9736\n",
      "Epoch 27 Loss 0.9629\n",
      "Time taken for 1 epoch 67.19339752197266 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.9004\n",
      "Epoch 28 Batch 100 Loss 0.9426\n",
      "Epoch 28 Batch 200 Loss 0.9804\n",
      "Epoch 28 Batch 300 Loss 0.9876\n",
      "Epoch 28 Batch 400 Loss 1.0004\n",
      "Epoch 28 Batch 500 Loss 0.9946\n",
      "Epoch 28 Loss 1.0141\n",
      "Time taken for 1 epoch 68.22421598434448 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.9215\n",
      "Epoch 29 Batch 100 Loss 0.9384\n",
      "Epoch 29 Batch 200 Loss 1.0071\n",
      "Epoch 29 Batch 300 Loss 1.0124\n",
      "Epoch 29 Batch 400 Loss 0.9713\n",
      "Epoch 29 Batch 500 Loss 0.9681\n",
      "Epoch 29 Loss 1.0325\n",
      "Time taken for 1 epoch 68.87706685066223 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.9654\n",
      "Epoch 30 Batch 100 Loss 0.9957\n",
      "Epoch 30 Batch 200 Loss 0.9999\n",
      "Epoch 30 Batch 300 Loss 1.0226\n",
      "Epoch 30 Batch 400 Loss 1.0262\n",
      "Epoch 30 Batch 500 Loss 1.0411\n",
      "Epoch 30 Loss 1.0293\n",
      "Time taken for 1 epoch 67.13284635543823 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.9358\n",
      "Epoch 31 Batch 100 Loss 1.0210\n",
      "Epoch 31 Batch 200 Loss 1.0027\n",
      "Epoch 31 Batch 300 Loss 1.0493\n",
      "Epoch 31 Batch 400 Loss 1.0063\n",
      "Epoch 31 Batch 500 Loss 1.0694\n",
      "Epoch 31 Loss 1.0546\n",
      "Time taken for 1 epoch 67.47865843772888 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 1.0079\n",
      "Epoch 32 Batch 100 Loss 1.0210\n",
      "Epoch 32 Batch 200 Loss 1.0527\n",
      "Epoch 32 Batch 300 Loss 1.1150\n",
      "Epoch 32 Batch 400 Loss 1.0786\n",
      "Epoch 32 Batch 500 Loss 1.0939\n",
      "Epoch 32 Loss 1.0846\n",
      "Time taken for 1 epoch 66.40557932853699 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 1.0367\n",
      "Epoch 33 Batch 100 Loss 1.0879\n",
      "Epoch 33 Batch 200 Loss 1.0812\n",
      "Epoch 33 Batch 300 Loss 1.0584\n",
      "Epoch 33 Batch 400 Loss 1.1323\n",
      "Epoch 33 Batch 500 Loss 1.0844\n",
      "Epoch 33 Loss 1.0739\n",
      "Time taken for 1 epoch 67.28367471694946 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 0 Loss 1.1030\n",
      "Epoch 34 Batch 100 Loss 1.1010\n",
      "Epoch 34 Batch 200 Loss 1.1035\n",
      "Epoch 34 Batch 300 Loss 1.0967\n",
      "Epoch 34 Batch 400 Loss 1.0998\n",
      "Epoch 34 Batch 500 Loss 1.0758\n",
      "Epoch 34 Loss 1.0933\n",
      "Time taken for 1 epoch 68.81805348396301 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 1.1512\n",
      "Epoch 35 Batch 100 Loss 1.1311\n",
      "Epoch 35 Batch 200 Loss 1.1469\n",
      "Epoch 35 Batch 300 Loss 1.1535\n",
      "Epoch 35 Batch 400 Loss 1.1275\n",
      "Epoch 35 Batch 500 Loss 1.1663\n",
      "Epoch 35 Loss 1.1618\n",
      "Time taken for 1 epoch 71.74766874313354 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 1.1053\n",
      "Epoch 36 Batch 100 Loss 1.1628\n",
      "Epoch 36 Batch 200 Loss 1.1479\n",
      "Epoch 36 Batch 300 Loss 1.1728\n",
      "Epoch 36 Batch 400 Loss 1.1866\n",
      "Epoch 36 Batch 500 Loss 1.1323\n",
      "Epoch 36 Loss 1.1580\n",
      "Time taken for 1 epoch 66.2769513130188 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 1.1236\n",
      "Epoch 37 Batch 100 Loss 1.1279\n",
      "Epoch 37 Batch 200 Loss 1.1595\n",
      "Epoch 37 Batch 300 Loss 1.2460\n",
      "Epoch 37 Batch 400 Loss 1.1737\n",
      "Epoch 37 Batch 500 Loss 1.1797\n",
      "Epoch 37 Loss 1.1340\n",
      "Time taken for 1 epoch 67.73306345939636 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 1.1897\n",
      "Epoch 38 Batch 100 Loss 1.1643\n",
      "Epoch 38 Batch 200 Loss 1.2093\n",
      "Epoch 38 Batch 300 Loss 1.2598\n",
      "Epoch 38 Batch 400 Loss 1.2338\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "          with tf.GradientTape() as tape:\n",
    "              # feeding the hidden state back into the model\n",
    "              # This is the interesting step\n",
    "              predictions = model(inp)\n",
    "              loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
    "              \n",
    "          grads = tape.gradient(loss, model.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "          if batch_n % 100 == 0:\n",
    "              template = 'Epoch {} Batch {} Loss {:.4f}'\n",
    "              print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
