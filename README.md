# Project 3 Generative Audio

Roy Jara, rjara@ucsd.edu
Elliott Lao, eylao@ucsd.edu

## Abstract

Include your abstract here. This should be one paragraph clearly describing your concept, method, and results. This should tell us what architecture/approach you used. Also describe your creative goals, and whether you were successful in achieving them. Also could describe future directions.

## Model/Data

Briefly describe the files that are included with your repository:
- CharRNN
- Tacotron2
- Wavenet
- training data 
  - reddit (r/oneliners) + joke repository (URL:https://www.kaggle.com/jiriroz/qa-jokes)
  - Laugh Wav files from a Kaggle Competition https://www.kaggle.com/c/freesound-audio-tagging/data

## Code

Your code for generating your project:
- Python: generative_code.py
- Jupyter notebooks: generative_code.ipynb

## Results

Documentation of your results in an appropriate format, both links to files and a brief description of their contents:
- `.wav` files or `.mp4`
- `.midi` files
- musical scores
- ... some other form

## Technical Notes

Any implementation details or notes we need to repeat your work. 
- Does this code require other pip packages, software, etc?
- Does it run on some other (non-datahub) platform? (CoLab, etc.)

## Reference

References to any papers, techniques, repositories you used:
- Papers
- Repositories
- Blog posts
